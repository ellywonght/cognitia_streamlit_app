import streamlit as st
import time
import random
import speech_recognition as sr
import mediapipe as mp
import cv2
import tempfile
import os

st.set_page_config(page_title="è…¦å‹è¨˜ Cognitia", layout="centered")

st.title("ğŸ§  è…¦å‹è¨˜ Cognitia")
st.markdown("å°ˆç‚ºæ—©æœŸé˜¿èŒ²æµ·é»˜ç—‡æª¢æ¸¬è€Œè¨­çš„AIè¼•æ‡‰ç”¨ã€‚è«‹ä¾æ¬¡å®Œæˆä»¥ä¸‹æ­¥é©Ÿï¼š")

if 'step' not in st.session_state:
    st.session_state.step = 0
if 'risk_score' not in st.session_state:
    st.session_state.risk_score = None

steps = ["èªéŸ³èƒ½åŠ›æ¸¬è©¦ï¼ˆèªéŸ³è¼¸å…¥ï¼‰", "åæ‡‰æ™‚é–“æ¸¬è©¦", "çœ¼çƒè¿½è¹¤åˆ†æ", "æ¨¡æ“¬ç³å­”åæ‡‰", "ç¶œåˆåˆ†æ"]

st.header(f"æ­¥é©Ÿ {st.session_state.step + 1}: {steps[st.session_state.step]}")

# Step 1: Speech Recognition
if st.session_state.step == 0:
    st.subheader("ğŸ¤ è«‹è¬›å‡ºï¼šä½ ä»Šæœé£Ÿå’—å’©æ—©é¤ï¼Ÿ")
    if st.button("é–‹å§‹éŒ„éŸ³"):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            st.info("éŒ„éŸ³ä¸­ï¼Œè«‹é–‹å§‹è¬›...")
            audio = recognizer.listen(source, timeout=5)
            st.success("éŒ„éŸ³å®Œæˆï¼Œæ­£åœ¨è¾¨è­˜...")
        try:
            result = recognizer.recognize_google(audio, language="zh-HK")
            st.write(f"èªéŸ³è¾¨è­˜çµæœï¼š{result}")
            st.session_state.step += 1
        except sr.UnknownValueError:
            st.error("ç„¡æ³•è¾¨è­˜èªéŸ³ï¼Œè«‹å†è©¦ä¸€æ¬¡ã€‚")

# Step 2: Reaction Time
elif st.session_state.step == 1:
    if 'reaction_start' not in st.session_state:
        if st.button("é–‹å§‹æ¸¬è©¦"):
            time.sleep(random.uniform(1, 3))
            st.session_state.reaction_start = time.time()
            st.success("è«‹é»æ“Šä¸‹é¢æŒ‰éˆ•ï¼")
            st.button("é»æˆ‘ï¼", on_click=lambda: st.session_state.update({'reaction_time': time.time() - st.session_state.reaction_start, 'step': st.session_state.step + 1}))
    elif 'reaction_time' in st.session_state:
        st.write(f"æ‚¨çš„åæ‡‰æ™‚é–“ï¼šç´„ {st.session_state.reaction_time:.2f} ç§’")

# Step 3: Eye Tracking
elif st.session_state.step == 2:
    st.info("ğŸ‘ï¸ å•Ÿå‹•é¡é ­é€²è¡Œç°¡æ˜“çœ¼çƒè¿½è¹¤åˆ†æ")
    run_eye_tracking = st.button("å•Ÿå‹•åˆ†æ")
    if run_eye_tracking:
        cap = cv2.VideoCapture(0)
        mp_face_mesh = mp.solutions.face_mesh.FaceMesh()
        stframe = st.empty()
        frame_count = 0
        while frame_count < 100:
            ret, frame = cap.read()
            if not ret:
                break
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = mp_face_mesh.process(rgb)
            if results.multi_face_landmarks:
                for face_landmarks in results.multi_face_landmarks:
                    for idx in [33, 133, 362, 263]:
                        x = int(face_landmarks.landmark[idx].x * frame.shape[1])
                        y = int(face_landmarks.landmark[idx].y * frame.shape[0])
                        cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)
            stframe.image(frame, channels="BGR")
            frame_count += 1
        cap.release()
        st.session_state.step += 1

# Step 4: PLR Simulation
elif st.session_state.step == 3:
    st.info("ğŸ”† æ¨¡æ“¬PLRï¼ˆç³å­”å…‰åæ‡‰ï¼‰ï¼šå‡è¨­å…‰æºé–ƒçˆ...")
    if st.button("å®Œæˆæ¨¡æ“¬"):
        st.session_state.step += 1

# Step 5: Final analysis
elif st.session_state.step == 4:
    if st.session_state.risk_score is None:
        st.session_state.risk_score = random.randint(20, 95)

    score = st.session_state.risk_score
    st.subheader(f"ğŸ§¾ ç¶œåˆé¢¨éšªåˆ†æ•¸ï¼š{score} / 100")
    if score >= 70:
        st.error("âš ï¸ é«˜é¢¨éšªï¼šå»ºè­°è«®è©¢è…¦ç¥ç¶“ç§‘é†«ç”Ÿ")
    elif score >= 40:
        st.warning("ğŸŸ¡ ä¸­åº¦é¢¨éšªï¼šå»ºè­°1å€‹æœˆå…§è¤‡æª¢")
    else:
        st.success("ğŸŸ¢ é¢¨éšªè¼ƒä½ï¼šå»ºè­°æ¯å¹´å®šæœŸæ¸¬è©¦")

    st.button("é‡æ–°é–‹å§‹", on_click=lambda: st.session_state.clear())